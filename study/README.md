# Study Line

### Work Division
Find the file in WeChat for more details.

:sparkles: Zekun Wang
- ALL Management

:sparkles: Zhaoze Sun
- Language Model : seq2seq, pre-train, etc.
- Text Generation
- Knowledge Fusion

:sparkles: Yakun Hou
- Knowledge Retrival : knowledge acquisition
- Fondamental Text Processing
- Dataset for Law

:sparkles: Yibo Zhang
- Knowledge Representation : fusable knowledge embedding, text2graph, 
- Text Matching : qa-pair, kq-pair, etc.
- Improvement

### Paper Reading

:sparkles: ALL

- [x] [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [x] [Weighted Transformer Network for Machine Translation](https://arxiv.org/abs/1711.02132)
- [x] [Natural Language QA Approaches using Reasoning with External Knowledge](https://arxiv.org/abs/2003.03446)
- [ ] [A Survey of Knowledge-Enhanced Text Generation](https://blender.cs.illinois.edu/paper/nlgsurvey2020.pdf)

:sparkles: Zekun Wang

- [x] [K-BERT: Enabling Language Representation with Knowledge Graph](https://arxiv.org/abs/1909.07606)
- [x] [Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains](https://arxiv.org/abs/2010.07717)
- [ ] [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/abs/1905.07129)
- [ ] [KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation](https://www.researchgate.net/publication/350418783_KEPLER_A_Unified_Model_for_Knowledge_Embedding_and_Pre-trained_Language_Representation)

:sparkles: Zhaoze Sun

- [x] [Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering](https://arxiv.org/abs/1911.03876)
- [x] [How Additional Knowledge can Improve Natural Language Commonsense Question Answering?](https://arxiv.org/abs/1909.08855)
- [ ] [KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation](https://www.researchgate.net/publication/350418783_KEPLER_A_Unified_Model_for_Knowledge_Embedding_and_Pre-trained_Language_Representation)
- [ ] [K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters](https://arxiv.org/abs/2002.01808)
- [ ] [Graph2Seq: Graph to Sequence Learning with Attention-based Neural Networks](https://arxiv.org/abs/1804.00823)

:sparkles: Yakun Hou

- [x] [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
- [x] [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)
- [ ] [PostKS: Learning to Select Knowledge for Response Generation in Dialog Systems](https://arxiv.org/pdf/1902.04911.pdf)

:sparkles: Yibo Zhang

- [x] [SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition](https://arxiv.org/abs/2006.01713)
- [ ] [SparseBERT: Rethinking the Importance Analysis in Self-attention](https://arxiv.org/abs/2102.12871)
- [ ] [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/abs/1905.07129)
- [ ] [KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation](https://www.researchgate.net/publication/350418783_KEPLER_A_Unified_Model_for_Knowledge_Embedding_and_Pre-trained_Language_Representation)
- [ ] [TransE: Translating Embeddings for Modeling Multi-relational Data](https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf)

### Code Learning

:sparkles: Zekun Wang

- [ ] PANpp
- [ ] PSGAN

:sparkles: Zhaoze Sun

- [ ] [BERT](https://github.com/jcyk/BERT)
- [ ] K-ADAPTER
- [ ] [Graph2Seq](https://github.com/graph4ai/graph4nlp/blob/master/graph4nlp/pytorch/models/graph2seq.py)

:sparkles: Yakun Hou

- [ ] RepMLP
- [ ] [PostKS](https://github.com/bzantium/Posterior-Knowledge-Selection)

:sparkles: Yibo Zhang

- [ ] [transformer](https://github.com/jayparks/transformer)
- [ ] ERNIE
- [ ] KEPLER
- [ ] [TransE](https://github.com/zqhead/TransE)
- [ ] [Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward](https://arxiv.org/abs/2005.01159)

### Code Practice
to be continued...
